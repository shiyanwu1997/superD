#!/usr/bin/env python3
"""
æµ‹è¯•è„šæœ¬ï¼šéªŒè¯æ—¥å¿—è·å–æ˜¯å¦å­˜åœ¨é‡å¤æˆ–ç¼ºæ¼é—®é¢˜
åŠŸèƒ½ï¼š
1. å¤šæ¬¡è·å–å†å²æ—¥å¿—
2. æ£€æŸ¥æ—¥å¿—æ˜¯å¦å­˜åœ¨é‡å¤æˆ–ç¼ºæ¼
3. éªŒè¯åç§»é‡è®¡ç®—çš„å‡†ç¡®æ€§
"""

import xmlrpc.client
import re
import urllib.parse

# Supervisorè¿æ¥ä¿¡æ¯
SUPERVISOR_HOST = "lb-dhoa2qv6-huedfymo7wbtk2pa.clb.ap-singapore.tencentclb.com"
SUPERVISOR_PORT = 9000
SUPERVISOR_USER = "supervisor"
SUPERVISOR_PASS = "C*3#E^*Kz@ggUM!EDMBQUC@xhLWGuzGbF6$KG"
PROGRAM_NAME = "axdev_api_queue_market"

# æ—¥å¿—ç›¸å…³å‚æ•°
MAX_LINES_PER_REQUEST = 50000  # æ¯æ¬¡è¯·æ±‚çš„æœ€å¤§å­—èŠ‚æ•°
TEST_ITERATIONS = 3  # æµ‹è¯•æ¬¡æ•°


def connect_supervisor():
    """è¿æ¥Supervisor API"""
    try:
        # æ­£ç¡®ç¼–ç URL
        encoded_pass = urllib.parse.quote(SUPERVISOR_PASS)
        url = f"http://{SUPERVISOR_USER}:{encoded_pass}@{SUPERVISOR_HOST}:{SUPERVISOR_PORT}/RPC2"
        
        proxy = xmlrpc.client.ServerProxy(url)
        # æµ‹è¯•è¿æ¥
        proxy.supervisor.getAPIVersion()
        print("âœ… æˆåŠŸè¿æ¥åˆ°Supervisor API")
        return proxy
    except Exception as e:
        print(f"âŒ è¿æ¥Supervisorå¤±è´¥: {e}")
        return None


def get_logs(proxy, program_name, offset, length):
    """è·å–æŒ‡å®šåç§»é‡å’Œé•¿åº¦çš„æ—¥å¿—"""
    try:
        if offset < 0:
            # è·å–æœ€æ–°æ—¥å¿—
            result = proxy.supervisor.tailProcessStdoutLog(program_name, 0, length)
            logs = result[0]
            new_offset = result[1]
        else:
            # è·å–å†å²æ—¥å¿—
            logs = proxy.supervisor.readProcessStdoutLog(program_name, offset, length)
            new_offset = offset
        
        # åˆ†å‰²æ—¥å¿—è¡Œä¸ºæ•°ç»„
        log_lines = [line.strip() for line in logs.strip().split('\n') if line.strip()]
        
        return log_lines, new_offset
    except Exception as e:
        print(f"âŒ è·å–æ—¥å¿—å¤±è´¥: {e}")
        return [], offset


def check_duplicates(log_lists):
    """æ£€æŸ¥å¤šä¸ªæ—¥å¿—åˆ—è¡¨ä¸­æ˜¯å¦å­˜åœ¨é‡å¤æ—¥å¿—"""
    print("\nğŸ” æ£€æŸ¥æ—¥å¿—é‡å¤é—®é¢˜:")
    print("-" * 60)
    
    # åˆå¹¶æ‰€æœ‰æ—¥å¿—è¡Œ
    all_lines = []
    for i, log_list in enumerate(log_lists):
        all_lines.extend(log_list)
        print(f"   ç¬¬{i+1}æ¬¡è·å–: {len(log_list)} è¡Œ")
    
    # æ£€æŸ¥é‡å¤
    unique_lines = set(all_lines)
    duplicate_count = len(all_lines) - len(unique_lines)
    
    if duplicate_count > 0:
        print(f"âŒ å‘ç° {duplicate_count} è¡Œé‡å¤æ—¥å¿—")
        
        # æ‰¾å‡ºé‡å¤çš„è¡Œ
        line_counts = {}
        for line in all_lines:
            line_counts[line] = line_counts.get(line, 0) + 1
        
        # æ‰“å°å‰10ä¸ªé‡å¤è¡Œ
        duplicates_found = 0
        for line, count in line_counts.items():
            if count > 1:
                print(f"   é‡å¤ {count} æ¬¡: {line[:100]}...")
                duplicates_found += 1
                if duplicates_found >= 10:
                    print("   ... æ›´å¤šé‡å¤è¡Œçœç•¥ ...")
                    break
        
        return False
    else:
        print(f"âœ… æœªå‘ç°é‡å¤æ—¥å¿— (å…± {len(all_lines)} è¡Œï¼Œ{len(unique_lines)} è¡Œå”¯ä¸€)")
        return True


def check_gaps(log_lists):
    """æ£€æŸ¥æ—¥å¿—æ˜¯å¦å­˜åœ¨æ—¶é—´é¡ºåºé—®é¢˜ï¼ˆå¯èƒ½è¡¨ç¤ºç¼ºæ¼ï¼‰"""
    print("\nğŸ“Š æ£€æŸ¥æ—¥å¿—æ—¶é—´é¡ºåºé—®é¢˜:")
    print("-" * 60)
    
    # åˆå¹¶æ‰€æœ‰æ—¥å¿—è¡Œ
    all_lines = []
    for log_list in log_lists:
        all_lines.extend(log_list)
    
    if len(all_lines) < 2:
        print("   æ—¥å¿—è¡Œå¤ªå°‘ï¼Œæ— æ³•æ£€æŸ¥é¡ºåº")
        return True
    
    # æ­£åˆ™è¡¨è¾¾å¼ï¼šåŒ¹é…æ—¥å¿—æ—¶é—´æˆ³
    time_pattern = re.compile(r'^(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{3})')
    
    # æ£€æŸ¥æ—¶é—´é¡ºåº
    prev_time = None
    out_of_order_count = 0
    
    for i, line in enumerate(all_lines):
        match = time_pattern.match(line)
        if match:
            current_time = match.group(1)
            
            if prev_time and current_time < prev_time:
                out_of_order_count += 1
                if out_of_order_count <= 5:  # åªæ‰“å°å‰5ä¸ªé—®é¢˜
                    print(f"   è¡Œ {i+1}: æ—¶é—´é¡ºåºé”™è¯¯")
                    print(f"      å½“å‰æ—¶é—´: {current_time}")
                    print(f"      å‰ä¸€æ—¶é—´: {prev_time}")
                    print(f"      è¡Œå†…å®¹: {line[:100]}...")
            
            prev_time = current_time
    
    if out_of_order_count > 0:
        print(f"âŒ å‘ç° {out_of_order_count} è¡Œæ—¶é—´é¡ºåºé”™è¯¯")
        print("   è¿™å¯èƒ½è¡¨ç¤ºæ—¥å¿—å­˜åœ¨ç¼ºæ¼æˆ–é‡å¤")
        return False
    else:
        print(f"âœ… æ‰€æœ‰æ—¥å¿—æŒ‰æ—¶é—´é¡ºåºæ­£ç¡®æ’åˆ—")
        return True


def check_offset_consistency(proxy, program_name):
    """æ£€æŸ¥åç§»é‡è®¡ç®—çš„ä¸€è‡´æ€§"""
    print("\nğŸ¯ æ£€æŸ¥åç§»é‡è®¡ç®—ä¸€è‡´æ€§:")
    print("-" * 60)
    
    # 1. è·å–æœ€æ–°æ—¥å¿—
    print("   1. è·å–æœ€æ–°æ—¥å¿—...")
    latest_logs, latest_offset = get_logs(proxy, program_name, -1, MAX_LINES_PER_REQUEST)
    print(f"      æœ€æ–°æ—¥å¿—è¡Œæ•°: {len(latest_logs)}")
    print(f"      æœ€æ–°åç§»é‡: {latest_offset}")
    
    # 2. è·å–å†å²æ—¥å¿—
    print("   2. è·å–å†å²æ—¥å¿—...")
    historical_offset = latest_offset - MAX_LINES_PER_REQUEST
    historical_logs, new_offset = get_logs(proxy, program_name, historical_offset, MAX_LINES_PER_REQUEST)
    print(f"      å†å²æ—¥å¿—è¡Œæ•°: {len(historical_logs)}")
    print(f"      è¯·æ±‚åç§»é‡: {historical_offset}")
    print(f"      è¿”å›åç§»é‡: {new_offset}")
    
    # 3. æ£€æŸ¥åç§»é‡æ˜¯å¦ä¸€è‡´
    if new_offset == historical_offset:
        print(f"      âœ… åç§»é‡ä¸€è‡´")
    else:
        print(f"      âŒ åç§»é‡ä¸ä¸€è‡´: è¯·æ±‚ {historical_offset}, è¿”å› {new_offset}")
    
    # 4. æ£€æŸ¥æ—¥å¿—å†…å®¹æ˜¯å¦è¿ç»­
    print("   3. æ£€æŸ¥æ—¥å¿—å†…å®¹è¿ç»­æ€§...")
    
    # åˆå¹¶æ—¥å¿—
    all_logs = historical_logs + latest_logs
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨è¿ç»­çš„æ—¥å¿—è¡Œ
    consecutive_count = 0
    for i in range(1, len(all_logs)):
        prev_line = all_logs[i-1]
        curr_line = all_logs[i]
        
        # ç®€å•æ£€æŸ¥ï¼šå¦‚æœä¸¤è¡Œçš„æ—¶é—´æˆ³ç›¸é‚»ï¼Œè®¤ä¸ºæ˜¯è¿ç»­çš„
        prev_match = re.search(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', prev_line)
        curr_match = re.search(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', curr_line)
        
        if prev_match and curr_match:
            prev_time = prev_match.group(1)
            curr_time = curr_match.group(1)
            
            if prev_time == curr_time:
                consecutive_count += 1
    
    if consecutive_count > 0:
        print(f"      âœ… å‘ç° {consecutive_count} ç»„è¿ç»­æ—¶é—´æˆ³çš„æ—¥å¿—è¡Œ")
    else:
        print(f"      âš ï¸  æœªå‘ç°è¿ç»­æ—¶é—´æˆ³çš„æ—¥å¿—è¡Œ")
    
    return True


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("æµ‹è¯•æ—¥å¿—è·å–æ˜¯å¦å­˜åœ¨é‡å¤æˆ–ç¼ºæ¼")
    print("=" * 60)
    
    # è¿æ¥Supervisor
    proxy = connect_supervisor()
    if not proxy:
        return
    
    # æµ‹è¯•1: å¤šæ¬¡è·å–æ—¥å¿—ï¼Œæ£€æŸ¥é‡å¤
    print("\n1. å¤šæ¬¡è·å–æ—¥å¿—ï¼Œæ£€æŸ¥é‡å¤é—®é¢˜")
    print("-" * 40)
    
    # è·å–æœ€æ–°æ—¥å¿—
    latest_logs, latest_offset = get_logs(proxy, PROGRAM_NAME, -1, MAX_LINES_PER_REQUEST)
    log_lists = [latest_logs]
    
    # å¤šæ¬¡è·å–å†å²æ—¥å¿—
    current_offset = latest_offset
    for i in range(TEST_ITERATIONS):
        historical_offset = max(0, current_offset - MAX_LINES_PER_REQUEST)
        historical_logs, current_offset = get_logs(proxy, PROGRAM_NAME, historical_offset, MAX_LINES_PER_REQUEST)
        log_lists.append(historical_logs)
    
    # æ£€æŸ¥é‡å¤
    check_duplicates(log_lists)
    
    # æµ‹è¯•2: æ£€æŸ¥æ—¥å¿—æ—¶é—´é¡ºåº
    print("\n2. æ£€æŸ¥æ—¥å¿—æ—¶é—´é¡ºåº")
    print("-" * 40)
    check_gaps(log_lists)
    
    # æµ‹è¯•3: æ£€æŸ¥åç§»é‡ä¸€è‡´æ€§
    print("\n3. æ£€æŸ¥åç§»é‡è®¡ç®—ä¸€è‡´æ€§")
    print("-" * 40)
    check_offset_consistency(proxy, PROGRAM_NAME)
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•å®Œæˆ")
    print("=" * 60)


if __name__ == "__main__":
    main()
